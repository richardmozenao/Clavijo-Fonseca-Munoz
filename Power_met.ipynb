{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#2. Método de las potencias\n",
        "\n"
      ],
      "metadata": {
        "id": "sjuAfF0aKW02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En adelante, cuando hablemos del **valor propio dominante** de una matriz, nos referimos al mayor valor propio en valor absoluto. El **método de las potencias** determina el valor propio dominante junto con su respectivo vector propio. Dicho método se basa en la multiplicación de un vector aproximado $x^(0)$ por una matriz $A$, con un \"escalamiento\" del vector resultante $y$ de tal modo que el factor de escalamiento se aproxima al valor propio dominante, mientras que el vector $y$ se aproxima al respectivo vector propio. La estrategia general de este método consiste en lo siguiente:\n",
        "\n",
        "<br>1. Considere una aproximación inicial $x^{(0)}$ para el vector propio $x$ asociado al valor propio dominante. Una de las componentes de $x^{(0)}$, que llamaremos **componente unidad**, debe ser igual a $1$.\n",
        "\n",
        "<br>2. Se lleva a cabo la operación $Ax^{(0)}$, y hacemos $y^{(1)}=Ax^{(0)}$.\n",
        "\n",
        "<br>3. \"Escalamos\" el vector $y^{(1)}$ para que la componente en la posición de la componente unidad siga siendo igual a $1$. Obtenemos entonces que $y^{(1)}=\\lambda^{(1)}x^{(1)}$.\n",
        "\n",
        "<br>Lo que sigue es repetir el procedimiento antes dado, a partir del paso 2, reemplazando el papel de  $x^{(0)}$  con el de  $x^{(1)}$, y continuar con las iteraciones. De este modo, tendremos en general que:\n",
        "\n",
        "<br>$$Ax^{(k)}=y^{(k+1)}=\\lambda^{(k+1)}x^{(k+1)}$$\n",
        "\n",
        "<br>Si la componente unidad es cero en alguna iteración, se debe cambiar la componente unidad de la aproximación inicial.\n",
        "\n",
        "<br>Profundicemos en algunos detalles: para aplicar el método de las potencias, supongamos que la matriz cuadrada $A$ tiene $n$ valores propios $\\lambda_1$,..., $\\lambda_n$, con un conjunto asociado de vectores propios linealmente independientes $\\{v_1,...,v_n\\}$. Si los vectores de dicho conjunto son linealmente dependientes, el método aún podría ser exitoso, pero ya no podemos asegurar que efectivamente lo es (de acuerdo con Burden). Adicionalmente, supongamos que:\n",
        "\n",
        "<br>$$|\\lambda_n| \\leq ... \\leq |\\lambda_2|<|\\lambda_1|$$\n",
        "\n",
        "<br>En palabras, estamos suponiendo que $A$ tiene exactamente un valor propio dominante $\\lambda_1$. Según Hoffman, la convergencia del método es lenta si hay presencia de valores propios cuyo valor absoluto es cercano al valor absoluto del valor propio dominante. Si no hay un único valor propio dominante, el método directamente falla.\n",
        "\n",
        "<br>Sea $x$ un vector de $\\mathbb R^n$ cualquiera. Como los vectores $v_1$,...,$v_n$ son linealmente independientes, podemos expresar a $x$ como combinación lineal de dichos vectores:\n",
        "\n",
        "<br>$$x=\\displaystyle \\sum_{i=1}^{n}C_iv_i$$\n",
        "\n",
        "<br>Luego:\n",
        "\n",
        "<br>--$Ax=A(\\displaystyle \\sum_{i=1}^{n}C_iv_i)=\\displaystyle \\sum_{i=1}^{n}C_i(Av_i)=\\displaystyle \\sum_{i=1}^{n}C_i(\\lambda_iv_i)$.\n",
        "\n",
        "<br>--$A^2x=A(Ax)=A(\\displaystyle \\sum_{i=1}^{n}C_i(\\lambda_iv_i))=\\displaystyle \\sum_{i=1}^{n}(C_i\\lambda_i)(Av_i)=\\displaystyle \\sum_{i=1}^{n}(C_i \\lambda_i)(\\lambda_iv_i)=\\displaystyle \\sum_{i=1}^{n}C_i(\\lambda_i^2v_i)$.\n",
        "\n",
        "<br>--$A^3x=A(A^2x)=A(\\displaystyle \\sum_{i=1}^{n}C_i(\\lambda_i^2v_i))=\\displaystyle \\sum_{i=1}^{n}(C_i\\lambda_i^2)(Av_i)=\\displaystyle \\sum_{i=1}^{n}(C_i \\lambda_i^2)(\\lambda_iv_i)=\\displaystyle \\sum_{i=1}^{n}C_i(\\lambda_i^3v_i)$.\n",
        "\n",
        "<br>En general:\n",
        "\n",
        "<br>$$A^{k}x=\\displaystyle \\sum_{i=1}^{n}C_i(\\lambda_i^{k}v_i)=\\lambda_1^{k}\\displaystyle \\sum_{i=1}^{n}(\\frac{\\lambda_i}{\\lambda_1})^kv_i$$\n",
        "\n",
        "<br>Como $\\lambda_1$ es el valor propio dominante, $|\\lambda_{i}/\\lambda_1|<1$, para todo i=2,...,n. Luego $(\\lambda_{i}/\\lambda_1)^k$ tiende a cero a medida que $k$ tiende a infinito. Por lo tanto:\n",
        "\n",
        "<br>$$\\lim \\limits_{k \\to \\infty}A^{k}x=\\lim \\limits_{k \\to \\infty}\\lambda_1^{k}(C_1v_1)$$\n",
        "\n",
        "<br>El límite anterior tiende a cero si $|\\lambda_1|<1$, y diverge si $1<|\\lambda_1|$. Por ésta razón es que debemos llevar a cabo el \"escalamiento\" entre iteraciones. Supongamos que partimos del vector inicial $x^{(0)}$ cuya componente unidad está en la primera coordenada $x_1$. Reemplzanado $x^{(0)}$ por $x$ en el procedimiento anterior, llegamos a que $A^kx^{(0)}=\\lambda_{1}^{k}C_1v_1=y^{(k)}$. Luego $y_1^{(k)}=\\lambda_1^kC_1$ ($y_1^{(k)}$ es la primera coordenada de $y^{(k)}$). Aplicando el procedimiento una vez más, llegamos a que $y_1^{(k+1)}=\\lambda_1^{k+1}C_1$. De modo pues que:\n",
        "\n",
        "<br>$$\\frac{y_1^{(k+1)}}{y_1^{(k)}}=\\frac{\\lambda_1^{k+1}C_1}{\\lambda_1^kC_1}=\\lambda_1$$\n",
        "\n",
        "<br>Así pues, si $y_1^{(k)}=1$, entonces $y_1^{(k+1)}=\\lambda_1$. Si escalamos $y_1^{(k+1)}$ por $\\lambda_1$ de tal modo que $y_1^{(k+1)}=1$, tendremos que $y_1^{(k+2)}=\\lambda_1$, y así sucesivamente. De esta forma, estamos \"factorizando\" $\\lambda_1$ entre iteraciones, logrando así que el límite que establecimos antes converja. En el límite, el factor de escalamiento se aproxima a $\\lambda_1$, mientras que el vector escalado se aproxima a su respectivo vector propio.\n",
        "\n",
        "<br>La siguiente relación establece una forma sencilla en la que el \"escalamiento\" entre iteraciones se puede llevar a cabo:\n",
        "\n",
        "<br>$$x^{(k+1)}=\\frac{Ax^{(k)}}{||Ax^{(k)}||_{\\infty}}$$\n",
        "\n",
        "<br>Algunas variaciones del método de las potencias permiten encontrar otros valores propios. Por ejemplo, el **método de las potencias inversas** encuentra el menor valor propio (en valor absoluto). Las **técnicas de deflación** permiten obtener aproximaciones para los otros valores propios de la matriz. La intuición de dichas técnicas es la siguiente: se forma una nueva matriz $B$, cuyos valores propios son iguales a los de $A$, a excepción del valor propio dominante de $A$, que es reemplazado por $0$ en $B$."
      ],
      "metadata": {
        "id": "RKBo6Rh1KaUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementación del método de las potencias"
      ],
      "metadata": {
        "id": "UgD8sUvvKkpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importamos las librerías necesarias.\n",
        "import numpy as np\n",
        "\n",
        "#Declaramos la función que aproximará el valor propio dominante y su correspondiente vector propio.\n",
        "\n",
        "def Power_method(matriz:np.array, vector:np.array, tolerance:float, max_number_iter:int):\n",
        "    #La función tomará como parámetros una matriz nxn, un vector n, una tolerancia y un número máximo de iteraciones.\n",
        "    k : int = 1 #Iniciamos un contador para saber el número de iteraciones que van hasta ese momento.\n",
        "    p : int = np.linalg.norm(vector, np.inf) #Calculamos la norma infinito del vector.\n",
        "    vector : np.array = vector / p  #Se divide cada valor del vector por la norma p y se toman como los nuevos valores del vector.\n",
        "    while k <= max_number_iter: #Se hacen las N iteraciones que se hayan declarado.\n",
        "        y : np.array = matriz.dot(vector) #Se multiplica la matriz por el vector correspondiente a esa iteración y se asigna ese\n",
        "                                          #nuevo vector a la variable y.\n",
        "        u : np.array = np.linalg.norm(y, np.inf) #A continuación, calculamos la norma infinito de ese nuevo vector para cada iteración.\n",
        "        if u == 0:\n",
        "            #Si la norma infinito de ese vector es cero, se retorna lo siguiente:\n",
        "            return (f\"Vector propio {vector}\"\n",
        "                    f\" La matriz {matriz} tiene valor propio 0, seleccione un nuevo vector propio vector x y reinicie la ejecución \")\n",
        "\n",
        "        new_vector : np.array = vector - (y/u)\n",
        "        #Declaramos un nuevo vector correspondiente al vector del error entre la iteración anterior y la que se está ejecutando.\n",
        "        Err : np.array = np.linalg.norm(new_vector, np.inf) #Calculamos la norma infinito de este nuevo vector.\n",
        "        vector : np.array = y / u #Reemplazamos el valor de la iteración anterior por los valores de la nueva iteración.\n",
        "        if Err < tolerance: #Si el error es menor a la tolerancia, retornamos el valor y vector propio dominantes.\n",
        "            return u, vector\n",
        "        k += 1 #Se incrementa el contador por cada iteración.\n",
        "    #Si el error no es menor que la tolerancia al cabo de las N iteraciones, se retorna lo siguiente:\n",
        "    return f\"El número máximo de iteraciones se ha excedido\""
      ],
      "metadata": {
        "id": "vrgT3XkHKmp0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Casos de prueba:"
      ],
      "metadata": {
        "id": "zQLIKVpIKyTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Declaramos los vectores correspondientes para los casos de prueba.\n",
        "vector = np.array([1, 1]) #Dimensión 2.\n",
        "vector1 = np.array([1, 2, 0]) #Dimensión 3.\n",
        "vector2 = np.array([1, 1, 1, 1]) #Dimensión 4.\n",
        "\n",
        "#Casos de prueba: declaramos un diccionario con las matrices de prueba.\n",
        "\n",
        "test = {\"A\": np.array([[2, 1], [3, 4]]),\n",
        "       \"B\": np.array([[3, 2], [3, 4]]),\n",
        "       \"C\": np.array([[2, 3], [1, 4]]),\n",
        "       \"D\": np.array([[1, 1, 2], [2, 1, 1],[1, 1, 3]]),\n",
        "       \"E\": np.array([[1, 1, 2], [2, 1, 3],[1, 1, 1]]),\n",
        "       \"F\": np.array([[2, 1, 2], [1, 1, 3],[1, 1, 1]]),\n",
        "       \"G\": np.array([[1, 1, 1, 2], [2, 1, 1, 1],[3, 2, 1, 2],[2, 1, 1, 4]]),\n",
        "        \"H\": np.array([[1, 2, 1, 2], [2, 1, 1, 1],[3, 2, 1, 2],[2, 1, 1, 4]])}\n",
        "\n",
        "for name, matriz in test.items(): # Iteramos sobre el diccionario con su respectiva clave y valor.\n",
        "    dimension : int = matriz.shape[0] # Obtenemos la dimensión de cada matriz del diccionario.\n",
        "    if dimension == 2: # Matrices de dimensión 2.\n",
        "        print(# Retornamos el correspondiente valor dominante a cada matriz, con su vector propio.\n",
        "            f\"El valor propio dominante asociado a la matriz {name} es: {Power_method(matriz, vector, tolerance=1e-5, max_number_iter=10)[0]}\"\n",
        "              f\" y su vector propio asociado es: {Power_method(matriz, vector, tolerance=1e-5, max_number_iter=10)[-1]}\")\n",
        "    if dimension == 3: # Matrices de dimensión 3.\n",
        "        print( # Retornamos el correspondiente valor dominante a cada matriz, con su vector propio.\n",
        "            f\"El valor propio dominante asociado a la matriz {name} es: {Power_method(matriz, vector1, tolerance=1e-12, max_number_iter=20)[0]}\"\n",
        "            f\" y su vector propio asociado es: {Power_method(matriz, vector1, tolerance=1e-10, max_number_iter=20)[-1]}\")\n",
        "    if dimension == 4: # Matrices de dimensión 4.\n",
        "        print( # Retornamos el correspondiente valor dominante a cada matriz, con su vector propio.\n",
        "            f\"El valor propio dominante asociado a la matriz {name} es: {Power_method(matriz,vector2 , tolerance=1e-5, max_number_iter=10)[0]}\"\n",
        "            f\" y su vector propio asociado es: {Power_method(matriz, vector2, tolerance=1e-5, max_number_iter=10)[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmdtv-q7Ky8W",
        "outputId": "a1cf7f8a-8c02-48b5-c0bb-c45a39014732"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El valor propio dominante asociado a la matriz A es: 5.000017066739485 y su vector propio asociado es: [0.33333447 1.        ]\n",
            "El valor propio dominante asociado a la matriz B es: 6.000017861289228 y su vector propio asociado es: [0.66666766 1.        ]\n",
            "El valor propio dominante asociado a la matriz C es: 5.0 y su vector propio asociado es: [1. 1.]\n",
            "El valor propio dominante asociado a la matriz D es: 4.5070186440933915 y su vector propio asociado es: [0.77812384 0.72889481 1.        ]\n",
            "El valor propio dominante asociado a la matriz E es: 4.048917339520433 y su vector propio asociado es: [0.69202147 1.         0.55495813]\n",
            "El valor propio dominante asociado a la matriz F es: 4.124885419764718 y su vector propio asociado es: [1.         0.90539067 0.60974737]\n",
            "El valor propio dominante asociado a la matriz G es: 6.634567462477316 y su vector propio asociado es: [0.60704991 0.54782282 0.87261933 1.        ]\n",
            "El valor propio dominante asociado a la matriz H es: 6.8272801484610675 y su vector propio asociado es: [0.68834475 0.56058636 0.88999093 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusiones\n",
        "\n",
        "<br>--**Ventajas y desventajas del método de las potencias:** Entre las ventajas del método de las potencias encontramos: es fácil y eficiente de implementar, dado que solo necesita operaciones básicas y el almacenamiento de pocos elementos (una matriz y un vector en cada iteración). Es especialmente útil para matrices grandes y **dispersas**. El hecho de encontrar el vector propio asociado al valor propio dominante también es una ventaja, pues los otros métodos solamente determinan valores propios.  Algunas desventajas: solamente encuentra el valor propio dominante. Requiere de matrices bien condicionadas y con espectros sencillos (se mencionó, por ejemplo, que si el valor propio dominante no está claramente separado de los demás, la convergencia del método podría ser lenta o inestable).  "
      ],
      "metadata": {
        "id": "b6RjysaOK7Cb"
      }
    }
  ]
}